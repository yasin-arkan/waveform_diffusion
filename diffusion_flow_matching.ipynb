{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasin-arkan/waveform_diff/blob/main/diffusion_flow_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GswAUDOqEpLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89eb704f-a48b-4da6-d222-e506a9cba0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dpntowngal47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d6bf9b-04c9-471c-a356-623760652ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1tNm9H2SeHlD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeeedd35-5013-4c47-beea-486c56fe7627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/diffusion\n"
          ]
        }
      ],
      "source": [
        "%cd gdrive/MyDrive/diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTN1LJfwnNgx"
      },
      "source": [
        "## Preparing the data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "twA8nVh7W_RV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67188146-3578-43b9-ddb4-25085286e670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1183, 129, 110)\n",
            "Before padding: torch.Size([1183, 129, 110])\n",
            "After padding: torch.Size([1183, 64, 128])\n",
            "Waveform shape: torch.Size([1120, 64, 128])\n",
            "Cond shape: torch.Size([1120, 4])\n",
            "Waveform shape (val): torch.Size([63, 64, 128])\n",
            "Cond shape (val): torch.Size([63, 4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from geopy import distance\n",
        "\n",
        "import librosa\n",
        "\n",
        "\n",
        "N_FFT = 256\n",
        "HOP_LENGTH = 64\n",
        "WIN_LENGTH = N_FFT\n",
        "\n",
        "file_path = \"data/timeseries_EW.csv\"\n",
        "\n",
        "def load_data(path, n_fft, hop_length, win_length, batch_size=16):\n",
        "\n",
        "    data = pd.read_csv(path)\n",
        "\n",
        "    cond_cols = ['Depth', 'Magnitude']\n",
        "    cond_vars = data[cond_cols].copy()\n",
        "\n",
        "    cond_data = []\n",
        "    norm_dict = {}\n",
        "\n",
        "    dist = source_site_distance(data)\n",
        "    cond_vars.loc[:, ('Distance')] = dist\n",
        "    cond_cols.append('Distance')\n",
        "\n",
        "    angle = compute_angle(data)\n",
        "    cond_vars.loc[:, ('Angle')] = angle\n",
        "    cond_cols.append('Angle')\n",
        "\n",
        "    for cvar in cond_cols:\n",
        "        cv = cond_vars[cvar].to_numpy()\n",
        "        cv = cv.reshape(cv.shape[0], 1)\n",
        "\n",
        "        cv_mean, cv_std = cv.mean(), cv.std()\n",
        "        norm_dict[cvar] = [cv_mean, cv_std]\n",
        "\n",
        "        cv = (cv - cv_mean) / cv_std\n",
        "\n",
        "        cond_data.append(cv)\n",
        "\n",
        "\n",
        "    wfs = data.iloc[:, 16:].to_numpy()\n",
        "\n",
        "    orig_wfs = wfs.copy()\n",
        "\n",
        "    wfs = librosa.stft(wfs, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
        "\n",
        "    wfs = np.abs(wfs)\n",
        "\n",
        "    print(wfs.shape)\n",
        "    wfs = torch.from_numpy(wfs).float()  # [1183, 128, 110]\n",
        "\n",
        "    print(\"Before padding:\", wfs.shape)\n",
        "    current_time_dim = wfs.shape[2] # Should be 110, we will pad it to 128\n",
        "    padding_needed = 128 - current_time_dim\n",
        "    time_padding = (0, padding_needed)\n",
        "    wfs = F.pad(wfs, time_padding, mode='constant', value=0)\n",
        "\n",
        "    wfs = wfs[:, :64, :]\n",
        "    print(\"After padding:\" ,wfs.shape) # Now it is [1183, 64, 128]\n",
        "\n",
        "\n",
        "    # We get the length for now and reshape the wfs to squeeze last 2 dimensions,\n",
        "    # so we can normalize them\n",
        "    length, x, y = wfs.shape\n",
        "\n",
        "    wfs = wfs.reshape((length, -1)) # wfs = [1183, 8192]\n",
        "\n",
        "    wfs_mean, wfs_std = wfs.mean(), wfs.std()\n",
        "    wfs = (wfs - wfs_mean) / wfs_std\n",
        "\n",
        "    wfs = wfs.reshape((length, x, y))\n",
        "\n",
        "    cond_var = np.concatenate(cond_data, axis=1)\n",
        "    cond_var = torch.from_numpy(cond_var)\n",
        "\n",
        "\n",
        "    train_dataset = STFTDataset(wfs[:1120, :, :], cond_var[:1120, :])\n",
        "    val_dataset = STFTDataset(wfs[1120:, :, :], cond_var[1120:, :])\n",
        "    all_dataset = STFTDataset(wfs, cond_var)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    all_dataloader = DataLoader(all_dataset,batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(\"Waveform shape:\", train_dataset.wfs.shape)\n",
        "    print(\"Cond shape:\", train_dataset.cond_var.shape)\n",
        "\n",
        "    print(\"Waveform shape (val):\", val_dataset.wfs.shape)\n",
        "    print(\"Cond shape (val):\", val_dataset.cond_var.shape)\n",
        "\n",
        "    return train_dataset, train_dataloader, val_dataset, val_dataloader, all_dataset, all_dataloader, wfs_mean, wfs_std, norm_dict, orig_wfs, cond_vars\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class STFTDataset(Dataset):\n",
        "    def __init__(self, wfs, cond_var):\n",
        "        # Ensure data is already scaled and in tensor format\n",
        "        self.wfs = wfs\n",
        "        self.cond_var = cond_var\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.wfs.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.wfs[idx], self.cond_var[idx]\n",
        "\n",
        "\n",
        "\n",
        "def source_site_distance(info):\n",
        "    '''compute the source-site distance'''\n",
        "    # reference: https://geopy.readthedocs.io/en/stable/#module-geopy.distance\n",
        "    # src_loc: (lat, long)\n",
        "\n",
        "    # source location: (lat, long)\n",
        "    src_lat = info.EventLat.to_numpy() # [#sample,]\n",
        "    src_lon = info.EventLon.to_numpy()\n",
        "\n",
        "    # station location: (lat, long)\n",
        "    station_lat = info.StationLat.to_numpy()\n",
        "    station_lon = info.StationLon.to_numpy()\n",
        "\n",
        "    # calculate the source-site distance\n",
        "    dist = []\n",
        "    for i in range(len(src_lat)):\n",
        "        dist_val = distance.distance((src_lat[i], src_lon[i]), (station_lat[i], station_lon[i])).km\n",
        "        dist.append(dist_val)\n",
        "\n",
        "    dist = np.array(dist).reshape(len(dist),1)\n",
        "\n",
        "    return dist\n",
        "\n",
        "\n",
        "\n",
        "def compute_angle(info):\n",
        "\n",
        "    '''\n",
        "    Calculate the angle between source centers and station locations\n",
        "    '''\n",
        "\n",
        "    # source location: (lat, long)\n",
        "    src_lat = info['EventLat'].to_numpy() # [#samples,]\n",
        "    src_lon = info['EventLon'].to_numpy()\n",
        "\n",
        "    # station location: (lat, long)\n",
        "    station_lat = info['StationLat'].to_numpy()\n",
        "    station_lon = info['StationLon'].to_numpy()\n",
        "\n",
        "    # calculate the source-site angle\n",
        "    angle = []\n",
        "    for i in range(len(src_lat)):\n",
        "        src_coord = (src_lat[i], src_lon[i])\n",
        "        station_coord = (station_lat[i], station_lon[i])\n",
        "\n",
        "        # Calculate the vector from the source center to the station location\n",
        "        vector = np.array(station_coord) - np.array(src_coord)\n",
        "\n",
        "        # Calculate the angle between the vectors and the x-axis (east direction)\n",
        "        angle_rad = np.arctan2(vector[1], vector[0])  # Angle in radians\n",
        "\n",
        "        # Convert the angle to degrees\n",
        "        angle_deg = np.degrees(angle_rad)\n",
        "\n",
        "        angle.append(angle_deg)\n",
        "\n",
        "    angle = np.array(angle).reshape(len(angle),1)\n",
        "\n",
        "    return angle\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataset, train_dataloader, val_dataset, val_dataloader, all_dataset, all_dataloader, wfs_mean, wfs_std, norm_dict, orig_wfs, cond_vars = load_data(file_path,\n",
        "                                                                                                                                                          N_FFT,\n",
        "                                                                                                                                                          HOP_LENGTH,\n",
        "                                                                                                                                                          WIN_LENGTH,\n",
        "                                                                                                                                                          batch_size=16)\n",
        "\n",
        "\n",
        "def plot_sample_stft(index):\n",
        "\n",
        "  spectrogram = train_dataset.wfs[index].cpu().numpy()\n",
        "\n",
        "  mean = wfs_mean.numpy()\n",
        "  std = wfs_std.numpy()\n",
        "\n",
        "  spectrogram = (spectrogram * std) + mean\n",
        "\n",
        "  tf = librosa.griffinlim(spectrogram, n_iter=512)\n",
        "\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "\n",
        "  ax1.set_title(f\"Waveform for sample {index}\")\n",
        "  ax1.set_xlabel(\"Time (seconds)\")\n",
        "  ax1.set_ylabel(\"Amplitude\")\n",
        "  ax1.plot(tf)\n",
        "\n",
        "\n",
        "  print(spectrogram.shape)\n",
        "  ax2.imshow(spectrogram, aspect='auto')\n",
        "  ax2.set_xlabel('Time Frame')\n",
        "  ax2.set_ylabel('Frequency Bin')\n",
        "  ax2.set_title(f\"Spectrogram for Sample {index}\")\n",
        "\n",
        "\n",
        "  ax3.set_title(\"Original waveform\")\n",
        "  orig = orig_wfs[index]\n",
        "  ax3.plot(orig)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# plot_sample_stft(333)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install flow_matching"
      ],
      "metadata": {
        "id": "BMZN-GmBA7mw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29207262-974b-4025-a825-974d077d4a5c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flow_matching in /usr/local/lib/python3.11/dist-packages (1.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flow_matching) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flow_matching) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.11/dist-packages (from flow_matching) (0.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flow_matching) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flow_matching) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq->flow_matching) (1.15.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flow_matching) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flow_matching\n",
        "from flow_matching.path.scheduler import CondOTScheduler\n",
        "from flow_matching.path import AffineProbPath\n",
        "from flow_matching.solver import Solver, ODESolver\n",
        "from flow_matching.utils import ModelWrapper\n",
        "\n",
        "path = AffineProbPath(scheduler=CondOTScheduler())\n"
      ],
      "metadata": {
        "id": "SWRZW-zsA1y1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv2RqizYnYCp"
      },
      "source": [
        "## Config, model and training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3ZmDQr2gSNBZ"
      },
      "outputs": [],
      "source": [
        "from diffusers import UNet2DModel, UNet2DConditionModel\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet2DConditionModel(\n",
        "    sample_size=(64, 128),\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    layers_per_block=2, # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"CrossAttnDownBlock2D\", # Use CrossAttn blocks where you want to inject conditioning\n",
        "        \"CrossAttnDownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"CrossAttnUpBlock2D\",\n",
        "        \"CrossAttnUpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        "\n",
        "    cross_attention_dim=512,\n",
        ")\n",
        "\n",
        "\n",
        "cond_emb = nn.Sequential(\n",
        "    nn.Linear(4, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 512)\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WrappedModel(ModelWrapper):\n",
        "\n",
        "    def __init__(self, model, num_train_timesteps):\n",
        "        super().__init__(model)\n",
        "        self.num_train_timesteps = num_train_timesteps\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor, h: torch.Tensor):\n",
        "        model_input_t = (t * (self.num_train_timesteps - 1)).long()\n",
        "        return self.model(x, model_input_t, h).sample"
      ],
      "metadata": {
        "id": "hx1lYbqZXiQU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Wi0Bo1fRi1Qk"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMPipeline, UNet2DConditionModel\n",
        "\n",
        "class CustomDDPMPipeline(DDPMPipeline):\n",
        "    def __init__(self, unet, scheduler):\n",
        "        super().__init__(unet, scheduler)\n",
        "\n",
        "    def __call__(self, batch_size=1, generator=None, num_inference_steps=None, output_type=\"pil\", return_dict=True, encoder_hidden_states=None):\n",
        "\n",
        "        # Sample gaussian noise to begin loop\n",
        "        image = torch.randn(\n",
        "            (batch_size, self.unet.config.in_channels, self.unet.config.sample_size[0], self.unet.config.sample_size[1]),\n",
        "            generator=generator,\n",
        "            )\n",
        "\n",
        "        image = image.to(self.device)\n",
        "        wrapped_unet = WrappedModel(self.unet, self.scheduler.config.num_train_timesteps)\n",
        "\n",
        "        solver = ODESolver(velocity_model=wrapped_unet)\n",
        "\n",
        "        T = torch.linspace(0, 1, 10)\n",
        "        T = T.to(self.device)\n",
        "\n",
        "\n",
        "        solver_args = {'h': encoder_hidden_states}\n",
        "        intermediate_samples = solver.sample(time_grid=T, x_init=image, method='midpoint', step_size=0.05, return_intermediates=True, **solver_args)\n",
        "        image = intermediate_samples[-1]\n",
        "\n",
        "        # image = (image / 2 + 0.5).clamp(0, 1)\n",
        "\n",
        "        if not return_dict:\n",
        "           return (image,), intermediate_samples, T\n",
        "\n",
        "        return (image,), intermediate_samples, T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "N-mw_fQYSf3s"
      },
      "outputs": [],
      "source": [
        "def evaluate(config, epoch, pipeline, hidden, original):\n",
        "    # Sample some images from random noise (this is the backward diffusion process).\n",
        "    # The default pipeline output type is `List[PIL.Image]`\n",
        "\n",
        "    # Output of the CustomDDPMPipeline above\n",
        "    images, samples, T = pipeline(\n",
        "        batch_size = config.eval_batch_size,\n",
        "        generator=torch.manual_seed(config.seed),\n",
        "        output_type=\"numpy\",\n",
        "        return_dict=False,\n",
        "        encoder_hidden_states=hidden\n",
        "    )\n",
        "\n",
        "    # Preparing the folders\n",
        "    arrs_dir =  os.path.join(config.output_dir, \"arrays\")\n",
        "    images_dir = os.path.join(config.output_dir, \"images\")\n",
        "    os.makedirs(arrs_dir, exist_ok=True)\n",
        "    os.makedirs(images_dir, exist_ok=True)\n",
        "    eval_batch_size = config.eval_batch_size\n",
        "\n",
        "\n",
        "    img_array = images[0].cpu().squeeze()\n",
        "\n",
        "\n",
        "    # Saving the output as an array here\n",
        "    np.save(os.path.join(arrs_dir, f\"epoch_{epoch:04d}_generated_sample.npy\"), img_array)\n",
        "\n",
        "\n",
        "    # Plotting the original and the output of the pipeline for comparison\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    original = original.cpu().numpy().squeeze()\n",
        "\n",
        "    ax1.set_title(f\"Original Sample, Epoch {epoch}\")\n",
        "    ax1.set_xlabel('Time Frame')\n",
        "    ax1.set_ylabel('Frequency Bin')\n",
        "    ax1.imshow(original, aspect='auto')\n",
        "\n",
        "    ax2.set_title(f\"Generated Sample, Epoch {epoch}\")\n",
        "    ax2.set_xlabel('Time Frame')\n",
        "    ax2.set_ylabel('Frequency Bin')\n",
        "    ax2.imshow(img_array, aspect='auto')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.savefig(os.path.join(images_dir, f\"epoch_{epoch:04d}_orig_vs_gen.png\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    # Plotting the generated image at each timestep (Optional) (Gaussian noise at t=0, final generated image at t=1)\n",
        "    # fig, axs = plt.subplots(2, 5,figsize=(20,10))\n",
        "\n",
        "    # samples = samples.cpu()\n",
        "\n",
        "    # axs = axs.flatten()\n",
        "\n",
        "    # for i, s in enumerate(samples):\n",
        "    #   s = s.squeeze()\n",
        "    #   axs[i].imshow(s, aspect='auto')\n",
        "    #   axs[i].axis('off')\n",
        "    #   axs[i].set_title(f't= %.2f' % (T[i]))\n",
        "\n",
        "    # plt.tight_layout()\n",
        "\n",
        "    # plt.savefig(os.path.join(images_dir, f\"epoch_{epoch:04d}_path_timesteps.png\"))\n",
        "    # plt.close()\n",
        "\n",
        "\n",
        "    # print(f\"Saved {eval_batch_size} sample images to {images_dir}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3glAitL_U30A"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "from accelerate.utils import ProjectConfiguration\n",
        "from huggingface_hub import create_repo, upload_folder\n",
        "from diffusers import DDPMPipeline\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "import os\n",
        "import random\n",
        "\n",
        "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, val_dataset, lr_scheduler):\n",
        "    # Initialize accelerator and tensorboard logging\n",
        "    logging_dir = os.path.join(config.output_dir, \"logs\")\n",
        "    accelerator_project_config = ProjectConfiguration(project_dir=config.output_dir, logging_dir=logging_dir)\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_config=accelerator_project_config,\n",
        "    )\n",
        "    if accelerator.is_main_process:\n",
        "        if config.push_to_hub:\n",
        "            repo_id = create_repo(\n",
        "                repo_id=Path(config.output_dir).name, exist_ok=True\n",
        "            ).repo_id\n",
        "        elif config.output_dir is not None:\n",
        "            os.makedirs(config.output_dir, exist_ok=True)\n",
        "        accelerator.init_trackers(\"train_example\")\n",
        "\n",
        "    # Prepare everything\n",
        "    # There is no specific order to remember, you just need to unpack the\n",
        "    # objects in the same order you gave them to the prepare method.\n",
        "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    id = random.randint(0, len(val_dataset) - 1)\n",
        "\n",
        "    # Now you train the model\n",
        "    for epoch in range(config.num_epochs):\n",
        "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            magnitudes, conds = batch\n",
        "\n",
        "            x_1 = magnitudes.to(accelerator.device).unsqueeze(1)\n",
        "            conds = conds.to(accelerator.device).float()\n",
        "            conds = cond_emb(conds)\n",
        "            conds = conds.unsqueeze(1)\n",
        "\n",
        "            # Sample noise to add to the images\n",
        "            x_0 = torch.randn_like(x_1).to(x_1.device)\n",
        "            bs = x_1.shape[0]\n",
        "\n",
        "            # Sample a random timestep for each image\n",
        "            t = torch.rand(x_1.shape[0]).to(x_1.device)\n",
        "            path_sample = path.sample(t=t, x_0=x_0, x_1=x_1)\n",
        "\n",
        "            # x_1 and x_0 : [batch_size, channels, width, height] - [16, 1, 64, 128]\n",
        "            # t : [batch_size] - [16]\n",
        "\n",
        "            with accelerator.accumulate(model):\n",
        "                # Predict the noise residual\n",
        "                # print(\"T:\",path_sample.t)\n",
        "                # print(\"T_scaled\", path_sample.t * noise_scheduler.config.num_train_timesteps - 1)\n",
        "\n",
        "                # Scaling the timesteps for UNet2DConditionModel\n",
        "                model_input_t = (path_sample.t * (noise_scheduler.config.num_train_timesteps - 1)).long()\n",
        "\n",
        "                predicted_velocity = model(path_sample.x_t, model_input_t, conds).sample\n",
        "                loss = torch.pow(predicted_velocity - path_sample.dx_t, 2).mean()\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
        "        if accelerator.is_main_process:\n",
        "            with torch.no_grad():\n",
        "              pipeline = CustomDDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
        "\n",
        "              wfs_val = val_dataset.wfs\n",
        "              cond_val = val_dataset.cond_var\n",
        "\n",
        "              sample_wfs = wfs_val[id].to(accelerator.device)\n",
        "\n",
        "              sample_cond = cond_val[id].to(accelerator.device).float()\n",
        "              sample_cond = cond_emb(sample_cond)\n",
        "              sample_cond = sample_cond.unsqueeze(0)\n",
        "              sample_cond = sample_cond.unsqueeze(0)\n",
        "\n",
        "\n",
        "              if (epoch) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1 or epoch == 0:\n",
        "                  evaluate(config, epoch, pipeline, sample_cond, sample_wfs)\n",
        "\n",
        "              if (epoch) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                  pipeline.save_pretrained(config.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8Tzuwh8ZzNHW"
      },
      "outputs": [],
      "source": [
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from dataclasses import dataclass\n",
        "from diffusers import FlowMatchEulerDiscreteScheduler\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    image_size = (64, 128)  # the generated image resolution\n",
        "    train_batch_size = 16\n",
        "    eval_batch_size = 1 # how many images to sample during evaluation\n",
        "    num_epochs = 120\n",
        "    gradient_accumulation_steps = 1\n",
        "    learning_rate = 1e-5\n",
        "    lr_warmup_steps = 500\n",
        "    save_image_epochs = 5\n",
        "    save_model_epochs = 15\n",
        "    mixed_precision = 'no'  # `no` for float32, `fp16` for automatic mixed precision\n",
        "    output_dir = 'checkpoints_EW_flow_test'  # the model namy locally and on the HF Hub\n",
        "    data_path = 'data/timeseries_EW.csv'\n",
        "\n",
        "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
        "    hub_private_repo = False\n",
        "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
        "    seed = 0\n",
        "\n",
        "config = TrainingConfig()\n",
        "\n",
        "\n",
        "noise_scheduler = FlowMatchEulerDiscreteScheduler(num_train_timesteps=1000)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v3bVHv-uukib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00260880-df51-41e7-e12a-2229ce38ac96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1183, 129, 110)\n",
            "Before padding: torch.Size([1183, 129, 110])\n",
            "After padding: torch.Size([1183, 64, 128])\n",
            "Waveform shape: torch.Size([1120, 64, 128])\n",
            "Cond shape: torch.Size([1120, 4])\n",
            "Waveform shape (val): torch.Size([63, 64, 128])\n",
            "Cond shape (val): torch.Size([63, 4])\n"
          ]
        }
      ],
      "source": [
        "file_path = config.data_path\n",
        "\n",
        "train_dataset, train_dataloader, val_dataset, val_dataloader, all_dataset, all_dataloader, wfs_mean, wfs_std, norm_dict, orig_wfs, cond_vars = load_data(file_path,\n",
        "                                                                        N_FFT,\n",
        "                                                                        HOP_LENGTH,\n",
        "                                                                        WIN_LENGTH,\n",
        "                                                                        batch_size=config.train_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j7PxtF975XL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc87f2c-29da-4586-9564-7a80687db9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1120, 64, 128])\n",
            "torch.Size([1120, 4])\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.wfs.shape)\n",
        "print(train_dataset.cond_var.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "\n",
        "print(\"Saving to:\", config.output_dir)\n",
        "print(\"Batch size:\", config.train_batch_size)\n",
        "\n",
        "args = (config, model, noise_scheduler, optimizer, train_dataloader, val_dataset, lr_scheduler)\n",
        "\n",
        "\n",
        "notebook_launcher(train_loop, args, num_processes=1, mixed_precision=config.mixed_precision)"
      ],
      "metadata": {
        "id": "-rippsYUoikA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464,
          "referenced_widgets": [
            "49862f17fa9148178d375749d2c7f32b",
            "d929dd02f0b64758a4a2d3494d83c761",
            "0f5a4bc0c94c47acaedcfb49fd7b8654",
            "d7c7bc9802404d1a99029f69f6a79265",
            "5265645a5e014b52a653fe7a8601142f",
            "b6948d114223407c9d0205e027252a32",
            "eb8f936d67ca4b708ce6ed35b490f32f",
            "129d346c6aa74a5c8c36fd397515eed2",
            "480ba687b2c341ac964c579a8f6b6e9f",
            "51badbb2b768431da64971a729a0eba7",
            "4442d62c1f6e48adac02fde3fb56a91f",
            "cb0c77c52da945caa9fe809180a3c243",
            "be8ae2dcf9824971bd6e3329223ba214",
            "6ffc32db31e9460c8e7d23f5ed2c8b22",
            "e7c949ee22b3406c878b7f4094020412",
            "6004c0abf6bf409e84e364f12871e78f",
            "8204dac9511c493ca0cb66cb99741de9",
            "ab50a5a67c6f40ffa6d616173821c78d",
            "eab7a8e73ff5456094bf0f3591dd9a51",
            "63c6f7c9263c45858401601ee6e8d892",
            "b6c3991521a44862ba0d4da39046600b",
            "9eb53b3c5c954991bdfb19ed1735b15f"
          ]
        },
        "outputId": "0770c388-c42f-471b-e187-06da42d8aac0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving to: checkpoints_EW_flow_test\n",
            "Batch size: 16\n",
            "Launching training on one GPU.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/70 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49862f17fa9148178d375749d2c7f32b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 1 sample images to checkpoints_EW_flow_test/images\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/70 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb0c77c52da945caa9fe809180a3c243"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-39-671426215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnotebook_launcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixed_precision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/launchers.py\u001b[0m in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Launching training on one CPU.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_processes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-38-167356579.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(config, model, noise_scheduler, optimizer, train_dataloader, val_dataset, lr_scheduler)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerate_step_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDistributedType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXLA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_xla_gradients_synced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    241\u001b[0m             )\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    244\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \"\"\"\n\u001b[0;32m--> 840\u001b[0;31m     if not torch.compiler.is_compiling() and not all(\n\u001b[0m\u001b[1;32m    841\u001b[0m         \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     ):\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \"\"\"\n\u001b[0;32m--> 840\u001b[0;31m     if not torch.compiler.is_compiling() and not all(\n\u001b[0m\u001b[1;32m    841\u001b[0m         \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     ):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run train.py"
      ],
      "metadata": {
        "id": "QseGaebjsM9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS5tbqcrmrJV"
      },
      "source": [
        "## MLP for max amplitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd_PS1ummqhN"
      },
      "outputs": [],
      "source": [
        "class MaxAmplitudePredictor(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc5(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh3p2j5gm1nQ"
      },
      "outputs": [],
      "source": [
        "wfs = train_dataset.wfs\n",
        "wfs_val = val_dataset.wfs\n",
        "\n",
        "wfs_flat = wfs.view(wfs.shape[0], -1)\n",
        "wfs_val_flat = wfs_val.view(wfs_val.shape[0], -1)\n",
        "\n",
        "train_max_amplitudes = torch.max(wfs_flat, dim=1, keepdims=True)[0]\n",
        "val_max_amplitudes = torch.max(wfs_val_flat, dim=1, keepdims=True)[0]\n",
        "\n",
        "print(train_max_amplitudes.shape)\n",
        "\n",
        "\n",
        "class MLPDataset(Dataset):\n",
        "    def __init__(self, cond_var, max_amps):\n",
        "        self.cond_var = cond_var\n",
        "        self.max_amps = max_amps\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.cond_var.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.cond_var[idx], self.max_amps[idx]\n",
        "\n",
        "mlp_train_dataset = MLPDataset(train_dataset.cond_var, train_max_amplitudes)\n",
        "mlp_val_dataset = MLPDataset(val_dataset.cond_var, val_max_amplitudes)\n",
        "\n",
        "mlp_train_dataloader = DataLoader(mlp_train_dataset, batch_size=config.train_batch_size, shuffle=True)\n",
        "mlp_val_dataloader = DataLoader(mlp_val_dataset, batch_size=config.eval_batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAzXfTMom9xW"
      },
      "outputs": [],
      "source": [
        "input_dim = mlp_train_dataset.cond_var.shape[1]\n",
        "mlp_model = MaxAmplitudePredictor(input_dim).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs_mlp = 1000\n",
        "\n",
        "for epoch in range(num_epochs_mlp):\n",
        "    mlp_model.train()\n",
        "    running_loss = 0.0\n",
        "    for conds, max_amps in mlp_train_dataloader:\n",
        "        conds = conds.to(device).float()\n",
        "        max_amps = max_amps.to(device)\n",
        "\n",
        "        mlp_optimizer.zero_grad()\n",
        "        outputs = mlp_model(conds)\n",
        "        loss = criterion(outputs, max_amps)\n",
        "        loss.backward()\n",
        "        mlp_optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * conds.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(mlp_train_dataset)\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      print(f\"MLP Epoch [{epoch+1}/{num_epochs_mlp}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Evaluate the MLP on the validation set\n",
        "    mlp_model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for conds, max_amps in mlp_val_dataloader:\n",
        "            conds = conds.to(device).float()\n",
        "            max_amps = max_amps.to(device)\n",
        "            outputs = mlp_model(conds)\n",
        "            loss = criterion(outputs, max_amps)\n",
        "            val_running_loss += loss.item() * conds.size(0)\n",
        "    val_epoch_loss = val_running_loss / len(mlp_val_dataset)\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      print(f\"MLP Validation Loss: {val_epoch_loss:.4f}\")\n",
        "\n",
        "print(\"MLP training finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugJDOrHowx2a"
      },
      "outputs": [],
      "source": [
        "example = val_dataset.cond_var[50]\n",
        "example_wf = val_dataset.wfs[50]\n",
        "\n",
        "max = example_wf.max()\n",
        "\n",
        "example = example.to(device).float()\n",
        "\n",
        "print(example)\n",
        "print(example.shape)\n",
        "preds = mlp_model(example)\n",
        "print(preds)\n",
        "print(max)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = 'max_amplitude.pth'\n",
        "\n",
        "torch.save(mlp_model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"MLP model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "cvU1AhsI9jtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = MaxAmplitudePredictor(input_dim)\n",
        "\n",
        "model_save_path = 'max_amplitude.pth'\n",
        "loaded_model.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "loaded_model.to(device)\n",
        "\n",
        "loaded_model.eval()\n",
        "\n",
        "p = loaded_model(example)\n",
        "print(p)\n",
        "print(max)"
      ],
      "metadata": {
        "id": "zWYbJagm9yNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzzCuSdRnglM"
      },
      "source": [
        "## Results of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVwCQbPCpHKJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7ynJVkCgFY_"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM85okJTsSg9"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ3Pgup5s24L"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "i = random.randint(0, len(val_dataset) - 1)\n",
        "print(i)\n",
        "\n",
        "h = val_dataset.cond_var[50].to(torch.device('cuda'))\n",
        "h = h.float()\n",
        "\n",
        "max_amplitude = mlp_model(h)\n",
        "max_amplitude = max_amplitude.detach().cpu().numpy()\n",
        "\n",
        "h = cond_emb(h)\n",
        "h = h.unsqueeze(0)\n",
        "h = h.unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  pipeline = CustomDDPMPipeline.from_pretrained(\"checkpoints_EW_flow_1e-4/\").to(\"cuda\")\n",
        "\n",
        "  images, samples, T = pipeline(batch_size = 1,\n",
        "                    generator = torch.manual_seed(config.seed),\n",
        "                    output_type = \"numpy\",\n",
        "                    return_dict = False,\n",
        "                    encoder_hidden_states=h)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 5,figsize=(20,10))\n",
        "\n",
        "samples = samples.cpu()\n",
        "\n",
        "axs = axs.flatten()\n",
        "\n",
        "for i, s in enumerate(samples):\n",
        "  s = s.squeeze()\n",
        "  axs[i].imshow(s, aspect='auto')\n",
        "  axs[i].axis('off')\n",
        "  axs[i].set_title(f't= %.2f' % (T[i]))\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N3zzzhA8pgzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoDVKUQvM7ut"
      },
      "outputs": [],
      "source": [
        "image = images[0]\n",
        "\n",
        "\n",
        "original = val_dataset.wfs[50].cpu().numpy()\n",
        "\n",
        "orig_max = original.max()\n",
        "\n",
        "im_1 = image[0].squeeze().cpu().numpy()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "mean = wfs_mean.cpu().numpy()\n",
        "std = wfs_std.cpu().numpy()\n",
        "\n",
        "im_1 = (im_1 * std) + mean\n",
        "original = (original * std) + mean\n",
        "\n",
        "im_1 *= max_amplitude\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"STD:\", std)\n",
        "print(\"Max amplitude:\", max_amplitude)\n",
        "print(\"Max amplitude (Original):\", orig_max)\n",
        "\n",
        "ax1.set_title(f\"Original\")\n",
        "ax1.set_xlabel('Time Frame')\n",
        "ax1.set_ylabel('Frequency Bin')\n",
        "orig = ax1.imshow(original, aspect='auto')\n",
        "fig.colorbar(orig, ax=ax1, label='Amplitude')\n",
        "\n",
        "ax2.set_title(f\"Generated\")\n",
        "ax2.set_xlabel('Time Frame')\n",
        "ax2.set_ylabel('Frequency Bin')\n",
        "gen = ax2.imshow(im_1, aspect='auto')\n",
        "fig.colorbar(gen, ax=ax2, label='Amplitude')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2HjbKqlORfY"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "\n",
        "wf_orig = librosa.griffinlim(original, n_iter = 512)\n",
        "wf_gen = librosa.griffinlim(im_1, n_iter = 512)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(f\"Generated waveform\")\n",
        "plt.xlabel(\"Time (miliseconds)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.ylim(wf_gen.min(), wf_gen.max())\n",
        "plt.plot(wf_gen)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(f\"Original waveform\")\n",
        "plt.xlabel(\"Time (miliseconds)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.ylim(wf_orig.min(), wf_orig.max())\n",
        "plt.plot(wf_orig)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM8x/qhf65GcfyY0W69Y421",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49862f17fa9148178d375749d2c7f32b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d929dd02f0b64758a4a2d3494d83c761",
              "IPY_MODEL_0f5a4bc0c94c47acaedcfb49fd7b8654",
              "IPY_MODEL_d7c7bc9802404d1a99029f69f6a79265"
            ],
            "layout": "IPY_MODEL_5265645a5e014b52a653fe7a8601142f"
          }
        },
        "d929dd02f0b64758a4a2d3494d83c761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6948d114223407c9d0205e027252a32",
            "placeholder": "",
            "style": "IPY_MODEL_eb8f936d67ca4b708ce6ed35b490f32f",
            "value": "Epoch0:100%"
          }
        },
        "0f5a4bc0c94c47acaedcfb49fd7b8654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_129d346c6aa74a5c8c36fd397515eed2",
            "max": 70,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_480ba687b2c341ac964c579a8f6b6e9f",
            "value": 70
          }
        },
        "d7c7bc9802404d1a99029f69f6a79265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51badbb2b768431da64971a729a0eba7",
            "placeholder": "",
            "style": "IPY_MODEL_4442d62c1f6e48adac02fde3fb56a91f",
            "value": "70/70[00:48&lt;00:00,1.41it/s,loss=0.64,lr=1.82e-6,step=69]"
          }
        },
        "5265645a5e014b52a653fe7a8601142f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6948d114223407c9d0205e027252a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb8f936d67ca4b708ce6ed35b490f32f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "129d346c6aa74a5c8c36fd397515eed2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "480ba687b2c341ac964c579a8f6b6e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51badbb2b768431da64971a729a0eba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4442d62c1f6e48adac02fde3fb56a91f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb0c77c52da945caa9fe809180a3c243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be8ae2dcf9824971bd6e3329223ba214",
              "IPY_MODEL_6ffc32db31e9460c8e7d23f5ed2c8b22",
              "IPY_MODEL_e7c949ee22b3406c878b7f4094020412"
            ],
            "layout": "IPY_MODEL_6004c0abf6bf409e84e364f12871e78f"
          }
        },
        "be8ae2dcf9824971bd6e3329223ba214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8204dac9511c493ca0cb66cb99741de9",
            "placeholder": "",
            "style": "IPY_MODEL_ab50a5a67c6f40ffa6d616173821c78d",
            "value": "Epoch1:36%"
          }
        },
        "6ffc32db31e9460c8e7d23f5ed2c8b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eab7a8e73ff5456094bf0f3591dd9a51",
            "max": 70,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63c6f7c9263c45858401601ee6e8d892",
            "value": 25
          }
        },
        "e7c949ee22b3406c878b7f4094020412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6c3991521a44862ba0d4da39046600b",
            "placeholder": "",
            "style": "IPY_MODEL_9eb53b3c5c954991bdfb19ed1735b15f",
            "value": "25/70[00:17&lt;00:31,1.45it/s,loss=2.1,lr=2.32e-6,step=94]"
          }
        },
        "6004c0abf6bf409e84e364f12871e78f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8204dac9511c493ca0cb66cb99741de9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab50a5a67c6f40ffa6d616173821c78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eab7a8e73ff5456094bf0f3591dd9a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63c6f7c9263c45858401601ee6e8d892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6c3991521a44862ba0d4da39046600b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eb53b3c5c954991bdfb19ed1735b15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}